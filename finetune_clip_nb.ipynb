{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c884ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision pandas scikit-learn open_clip_torch[training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6232a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "INDEX_CSV = 'index.csv'     # existing manual captions\n",
    "TRAIN_ORIGINAL_CSV = 'train_original.csv'\n",
    "AUGMENTED_TRAIN = \"train.csv\"\n",
    "VAL_CSV = 'val.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e0d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 16\n",
      "Train samples: 12 | Validation samples: 4\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INDEX_CSV)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "df.head()\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train samples: {len(train_df)} | Validation samples: {len(val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fe99ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train_original.csv and val.csv\n",
      "Columns in  index.csv:  ['filepath', 'caption']\n",
      "Columns in train_original.csv: ['filepath', 'caption']\n",
      "Columns in val.csv: ['filepath', 'caption']\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(TRAIN_ORIGINAL_CSV, index=False)\n",
    "val_df.to_csv(VAL_CSV, index=False)\n",
    "print(f\"Saved {TRAIN_ORIGINAL_CSV} and {VAL_CSV}\")\n",
    "df_train = pd.read_csv(TRAIN_ORIGINAL_CSV)\n",
    "df_val = pd.read_csv(VAL_CSV)\n",
    "\n",
    "print(\"Columns in  index.csv: \",df.columns.tolist())\n",
    "print(\"Columns in train_original.csv:\", df_train.columns.tolist())\n",
    "print(\"Columns in val.csv:\", df_val.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c0505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing local llm (llama-cpp-python) dependencies..\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dependencies done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\coolb\\anaconda3\\envs\\clip-browser\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\coolb\\anaconda3\\envs\\clip-browser\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\coolb\\anaconda3\\envs\\clip-browser\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "print(\"installing local llm (llama-cpp-python) dependencies..\")\n",
    "%pip install llama-cpp-python huggingface_hub --quiet\n",
    "print(\"Dependencies done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54c99b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "print(device)\n",
    "CUDA_PATH = os.environ.get(\"CUDA_PATH\")\n",
    "print(CUDA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fca57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running LLM-based Augmentation ---\n",
      "LLM model 'Phi-3-mini-4k-instruct-q4.gguf' already exists at local_llm_models\\Phi-3-mini-4k-instruct-q4.gguf.\n",
      "Local LLM 'Phi-3-mini-4k-instruct-q4.gguf' loaded successfully.\n",
      "Starting text augmentation using 'llm' method...\n",
      "Generated 2 paraphrases for: 'church x3'\n",
      "Generated 2 paraphrases for: 'street at night'\n",
      "Generated 2 paraphrases for: 'standing under lamp'\n",
      "Generated 2 paraphrases for: 'church blurry'\n",
      "Generated 2 paraphrases for: 'me again'\n",
      "Generated 2 paraphrases for: 'sky at university'\n",
      "Generated 2 paraphrases for: 'joel sitting on a rock at night'\n",
      "Generated 2 paraphrases for: 'boat during night'\n",
      "Generated 2 paraphrases for: 'andother church'\n",
      "Generated 2 paraphrases for: 'henry'\n",
      "Generated 2 paraphrases for: 'me starting down'\n",
      "Generated 2 paraphrases for: 'standing under lamp'\n",
      "Text augmentation complete. Saved augmented train data to train.csv with 36 entries.\n",
      "Final train.csv has 36 entries.\n",
      "               filepath                                     caption\n",
      "0  dataset/IMG_0825.JPG  Person positioned beneath a bright source.\n",
      "1  dataset/IMG_0718.JPG                   Revisiting the same scene\n",
      "2  dataset/church_2.JPG                 Additional place of worship\n",
      "3  dataset/IMG_0794.JPG                            me starting down\n",
      "4   dataset/the_sky.JPG                         University skyline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "#manual augment (type synonyms for captions)\n",
    "#print(\"\\n--- Running Manual Synonym Augmentation ---\")\n",
    "# !python text_augment.py --augmentation_method manual --num_aug_per_original 5\n",
    "\n",
    "#llm augmentation\n",
    "print(\"\\n--- Running LLM-based Augmentation ---\")\n",
    "!python text_augment.py --augmentation_method llm --num_aug_per_original 3\n",
    "\n",
    "train_augmented_df = pd.read_csv(AUGMENTED_TRAIN)\n",
    "print(f\"Final train.csv has {len(train_augmented_df)} entries.\")\n",
    "print(train_augmented_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b22c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND:\n",
      "python -m open_clip_train.main --train-data train.csv --val-data val.csv --csv-img-key filepath --csv-caption-key caption --csv-separator , --model ViT-B-32 --pretrained openai --report-to tensorboard --log-every-n-steps 50 --batch-size 32 --lr 1e-4 --epochs 10 --warmup 10000 --workers 4 --device cuda --use_timm true --aug-cfg scale=(0.4,1.0) ratio=(0.75,1.3333333333333333) color_jitter=(0.4,0.4,0.4,0.1) color_jitter_prob=0.8 re_prob=0.25 re_count=1 gray_scale_prob=0.2\n",
      "STDOUT:\n",
      " \n",
      "STDERR:\n",
      " usage: main.py [-h] [--train-data TRAIN_DATA]\n",
      "               [--train-data-upsampling-factors TRAIN_DATA_UPSAMPLING_FACTORS]\n",
      "               [--val-data VAL_DATA] [--train-num-samples TRAIN_NUM_SAMPLES]\n",
      "               [--val-num-samples VAL_NUM_SAMPLES]\n",
      "               [--dataset-type {webdataset,csv,synthetic,auto}]\n",
      "               [--dataset-resampled] [--csv-separator CSV_SEPARATOR]\n",
      "               [--csv-img-key CSV_IMG_KEY] [--csv-caption-key CSV_CAPTION_KEY]\n",
      "               [--imagenet-val IMAGENET_VAL] [--imagenet-v2 IMAGENET_V2]\n",
      "               [--cache-dir CACHE_DIR] [--logs LOGS] [--log-local]\n",
      "               [--name NAME] [--workers WORKERS] [--batch-size BATCH_SIZE]\n",
      "               [--epochs EPOCHS] [--epochs-cooldown EPOCHS_COOLDOWN] [--lr LR]\n",
      "               [--beta1 BETA1] [--beta2 BETA2] [--eps EPS] [--wd WD]\n",
      "               [--momentum MOMENTUM] [--warmup WARMUP] [--opt OPT]\n",
      "               [--use-bn-sync] [--skip-scheduler]\n",
      "               [--lr-scheduler LR_SCHEDULER]\n",
      "               [--lr-cooldown-end LR_COOLDOWN_END]\n",
      "               [--lr-cooldown-power LR_COOLDOWN_POWER]\n",
      "               [--save-frequency SAVE_FREQUENCY] [--save-most-recent]\n",
      "               [--zeroshot-frequency ZEROSHOT_FREQUENCY]\n",
      "               [--val-frequency VAL_FREQUENCY] [--resume RESUME]\n",
      "               [--precision {amp,amp_bf16,amp_bfloat16,bf16,fp16,pure_bf16,pure_fp16,fp32}]\n",
      "               [--model MODEL] [--pretrained PRETRAINED] [--pretrained-image]\n",
      "               [--lock-image]\n",
      "               [--lock-image-unlocked-groups LOCK_IMAGE_UNLOCKED_GROUPS]\n",
      "               [--lock-image-freeze-bn-stats] [--image-mean MEAN [MEAN ...]]\n",
      "               [--image-std STD [STD ...]]\n",
      "               [--image-interpolation {bicubic,bilinear,random}]\n",
      "               [--image-resize-mode {shortest,longest,squash}]\n",
      "               [--aug-cfg [AUG_CFG ...]] [--grad-checkpointing] [--local-loss]\n",
      "               [--gather-with-grad]\n",
      "               [--force-context-length FORCE_CONTEXT_LENGTH]\n",
      "               [--force-image-size FORCE_IMAGE_SIZE [FORCE_IMAGE_SIZE ...]]\n",
      "               [--force-quick-gelu]\n",
      "               [--force-patch-dropout FORCE_PATCH_DROPOUT]\n",
      "               [--force-custom-text] [--torchscript] [--torchcompile]\n",
      "               [--trace] [--accum-freq ACCUM_FREQ] [--device DEVICE]\n",
      "               [--dist-url DIST_URL] [--dist-backend DIST_BACKEND]\n",
      "               [--report-to REPORT_TO] [--wandb-notes WANDB_NOTES]\n",
      "               [--wandb-project-name WANDB_PROJECT_NAME] [--debug]\n",
      "               [--copy-codebase] [--horovod] [--ddp-static-graph]\n",
      "               [--no-set-device-rank] [--seed SEED]\n",
      "               [--grad-clip-norm GRAD_CLIP_NORM] [--lock-text]\n",
      "               [--lock-text-unlocked-layers LOCK_TEXT_UNLOCKED_LAYERS]\n",
      "               [--lock-text-freeze-layer-norm]\n",
      "               [--log-every-n-steps LOG_EVERY_N_STEPS]\n",
      "               [--coca-caption-loss-weight COCA_CAPTION_LOSS_WEIGHT]\n",
      "               [--coca-contrastive-loss-weight COCA_CONTRASTIVE_LOSS_WEIGHT]\n",
      "               [--remote-sync REMOTE_SYNC]\n",
      "               [--remote-sync-frequency REMOTE_SYNC_FREQUENCY]\n",
      "               [--remote-sync-protocol {s3,fsspec}]\n",
      "               [--delete-previous-checkpoint] [--distill-model DISTILL_MODEL]\n",
      "               [--distill-pretrained DISTILL_PRETRAINED]\n",
      "               [--use-bnb-linear USE_BNB_LINEAR] [--siglip]\n",
      "               [--loss-dist-impl LOSS_DIST_IMPL]\n",
      "main.py: error: unrecognized arguments: --use_timm true\n",
      "\n",
      "Exit code: 2\n"
     ]
    }
   ],
   "source": [
    "aug_cfg_args = [\n",
    "    \"use_timm=True\",\n",
    "    \"scale=(0.4,1.0)\",\n",
    "    \"ratio=(0.75,1.3333333333333333)\",\n",
    "    \"color_jitter=(0.4,0.4,0.4,0.1)\",\n",
    "    \"color_jitter_prob=0.8\",\n",
    "    \"re_prob=0.25\",\n",
    "    \"re_count=1\",\n",
    "    \"gray_scale_prob=0.2\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"-m\", \"open_clip_train.main\",\n",
    "    \"--train-data\", \"train.csv\",\n",
    "    \"--val-data\", \"val.csv\",\n",
    "    \"--csv-img-key\", \"filepath\",\n",
    "    \"--csv-caption-key\", \"caption\",\n",
    "    \"--csv-separator\", \",\",\n",
    "    \"--model\", \"ViT-B-32\",\n",
    "    \"--pretrained\", \"openai\",\n",
    "    \"--report-to\", \"tensorboard\",\n",
    "    \"--log-every-n-steps\", \"50\",\n",
    "    \"--batch-size\", \"32\",\n",
    "    \"--lr\", \"1e-4\",\n",
    "    \"--epochs\", \"10\",\n",
    "    \"--warmup\", \"10000\",\n",
    "    \"--workers\", \"4\",\n",
    "    \"--device\", device,\n",
    "    \"--aug-cfg\"\n",
    "] + aug_cfg_args\n",
    "\n",
    "# Print the command for verification\n",
    "print(\"COMMAND:\")\n",
    "print(\" \".join(cmd))\n",
    "\n",
    "# Run it\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "# Output logs\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)\n",
    "print(\"Exit code:\", result.returncode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip-browser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
